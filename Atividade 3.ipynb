{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# Criando as bases RAW\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "# =========================\n# RAW CSV/TXT/TSV  ->  RAW Parquet (mesmo prefixo RAW)\n# Converte TODOS os arquivos de RAW. Força \\t para .tsv.\n# === Versão com detecção de ENCODING + fallbacks robustos ===\n# =========================\nimport sys, re, unicodedata, csv, boto3\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F, types as T\n\n# ---------- Bootstrap Glue ----------\nspark = None\ntry:\n    from awsglue.utils import getResolvedOptions\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    from pyspark.context import SparkContext\n\n    sc = SparkContext.getOrCreate()\n    glueContext = GlueContext(sc)\n    spark = glueContext.spark_session\n    args = getResolvedOptions(\n        sys.argv,\n        [\"JOB_NAME\", \"BUCKET\", \"RAW_PREFIX\", \"NORMALIZE_COLUMNS\", \"FLATTEN\", \"OVERWRITE_EXISTING\"]\n    )\n    job = Job(glueContext); job.init(args[\"JOB_NAME\"], args)\nexcept Exception:\n    # Fallback p/ notebook\n    spark = SparkSession.builder.appName(\"raw_to_parquet_all\").getOrCreate()\n    args = {\n        \"BUCKET\": \"bucket-etl-edb011\",\n        \"RAW_PREFIX\": \"raw\",\n        \"NORMALIZE_COLUMNS\": \"true\",\n        \"FLATTEN\": \"true\",\n        \"OVERWRITE_EXISTING\": \"true\",\n    }\n\n# ---------- Parâmetros ----------\nBUCKET   = args.get(\"BUCKET\")\nRAW_PREF = args.get(\"RAW_PREFIX\", \"raw\").strip(\"/\")\nDO_NORM  = args.get(\"NORMALIZE_COLUMNS\", \"true\").lower() == \"true\"\nDO_FLAT  = args.get(\"FLATTEN\", \"true\").lower() == \"true\"\nOVERWRITE= args.get(\"OVERWRITE_EXISTING\", \"true\").lower() == \"true\"\n\nif not BUCKET or not RAW_PREF:\n    raise RuntimeError(\"Informe --BUCKET e --RAW_PREFIX.\")\n\nbucket_uri = f\"s3://{BUCKET}\"\nraw_uri    = f\"{bucket_uri}/{RAW_PREF}\"\n\nprint(f\"[INFO] BUCKET={BUCKET} | RAW=s3://{BUCKET}/{RAW_PREF}/ | NORMALIZE_COLUMNS={DO_NORM} | FLATTEN={DO_FLAT} | OVERWRITE_EXISTING={OVERWRITE}\")\n\ns3c = boto3.client(\"s3\")\n\n# ---------- Detecção de encoding ----------\ntry:\n    import chardet\n    _HAVE_CHARDET = True\nexcept Exception:\n    _HAVE_CHARDET = False\n\ndef _normalize_charset(name: str) -> str:\n    \"\"\"Mapeia nomes diversos para o que o Spark/Java aceita melhor.\"\"\"\n    if not name:\n        return \"UTF-8\"\n    n = name.strip().lower().replace(\"_\", \"-\")\n    if n in (\"utf-8\", \"utf8\", \"utf-8-sig\", \"utf8-sig\", \"ascii\"):\n        # ascii é subconjunto de utf-8\n        return \"UTF-8\"\n    if n in (\"iso-8859-1\", \"latin-1\", \"latin1\"):\n        return \"ISO-8859-1\"\n    if n in (\"cp1252\", \"windows-1252\", \"windows1252\"):\n        return \"windows-1252\"\n    # fallback genérico: tenta usar em Java tal como está (pode funcionar)\n    return name\n\ndef detect_encoding_s3(key: str, sample_bytes: int = 65536) -> tuple[str, float]:\n    \"\"\"\n    Lê um pedaço do arquivo no S3 e tenta detectar o encoding.\n    Retorna (encoding_normalizado, confiança[0..1]).\n    \"\"\"\n    try:\n        obj = s3c.get_object(Bucket=BUCKET, Key=key, Range=f\"bytes=0-{sample_bytes}\")\n        raw = obj[\"Body\"].read()\n        if _HAVE_CHARDET:\n            res = chardet.detect(raw)\n            enc = _normalize_charset(res.get(\"encoding\") or \"UTF-8\")\n            conf = float(res.get(\"confidence\") or 0.0)\n            return enc, conf\n        else:\n            # Sem chardet, assume UTF-8 e deixa fallback cuidar do resto\n            return \"UTF-8\", 0.0\n    except Exception:\n        return \"UTF-8\", 0.0\n\n# ---------- Listagem: inclui .csv, .txt e .tsv ----------\ndef list_raw_files(prefix: str):\n    cont = None\n    while True:\n        kw = {\"Bucket\": BUCKET, \"Prefix\": prefix}\n        if cont: kw[\"ContinuationToken\"] = cont\n        resp = s3c.list_objects_v2(**kw)\n        for o in resp.get(\"Contents\", []):\n            k = o[\"Key\"]\n            if not k.endswith(\"/\") and k.lower().endswith((\".csv\", \".txt\", \".tsv\")):\n                yield k\n        if resp.get(\"IsTruncated\"):\n            cont = resp.get(\"NextContinuationToken\")\n        else:\n            break\n\n# ---------- Util: normalizar nomes de coluna (remove BOM e acentos) ----------\ndef to_snake(s: str) -> str:\n    # Remove BOM se aparecer no início\n    s = s.lstrip(\"\\ufeff\")\n    s = unicodedata.normalize(\"NFKD\", s)\n    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n    s = re.sub(r\"\\s+\", \"_\", s.strip())\n    s = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", s)\n    s = re.sub(r\"_+\", \"_\", s)\n    return s.lower().strip(\"_\")[:64]\n\n# ---------- Inferência de delimitador (usa encoding detectado); força \\t para .tsv ----------\ndef infer_delimiter_s3(key: str, encoding_hint: str | None = None, sample_bytes: int = 65536) -> str:\n    low = key.lower()\n    if low.endswith(\".tsv\"):\n        return \"\\t\"  # TSV = tab\n    try:\n        obj = s3c.get_object(Bucket=BUCKET, Key=key, Range=f\"bytes=0-{sample_bytes}\")\n        head = obj[\"Body\"].read()\n        enc = encoding_hint or detect_encoding_s3(key, sample_bytes=sample_bytes)[0]\n        sample = head.decode(enc, errors=\"ignore\")\n    except Exception:\n        sample = \"\"\n\n    try:\n        dialect = csv.Sniffer().sniff(sample, delimiters=[\",\", \";\", \"|\", \"\\t\"])\n        return dialect.delimiter\n    except Exception:\n        # heurística simples\n        c_comma = sample.count(\",\")\n        c_semi  = sample.count(\";\")\n        c_pipe  = sample.count(\"|\")\n        if c_semi >= max(c_comma, c_pipe): return \";\"\n        if c_pipe > c_comma: return \"|\"\n        if \"\\t\" in sample: return \"\\t\"\n        return \",\"\n\ndef normalize_columns_df(df):\n    if not DO_NORM:\n        return df\n    cols = []\n    for c in df.columns:\n        if c is None:\n            continue\n        name = str(c).lstrip(\"\\ufeff\")  # remove BOM se vier do header\n        if name.lower().startswith(\"unnamed\"):\n            continue\n        cols.append(F.col(c).alias(to_snake(name)))\n    df = df.select(*cols)\n    # força string (seguro p/ tipos heterogêneos)\n    for c in df.columns:\n        df = df.withColumn(c, F.col(c).cast(T.StringType()))\n    return df\n\ndef write_parquet_in_raw(df, base_no_ext: str):\n    \"\"\"\n    Se FLATTEN=true: escreve em RAW/__tmp/<base>/ e copia part-*.parquet para RAW/<base>.parquet\n    Se FLATTEN=false: escreve em RAW/<base>/ (sem copiar) — padrão Spark.\n    \"\"\"\n    if DO_FLAT:\n        tmp_dir = f\"{raw_uri}/__tmp/{base_no_ext}/\"\n        df.coalesce(1).write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(tmp_dir)\n        print(f\"[WRITE] tmp: {tmp_dir}\")\n\n        # localizar part-*.parquet e copiar para RAW/<base>.parquet\n        part_key = None\n        prefix_tmp = f\"{RAW_PREF}/__tmp/{base_no_ext}/\"\n        resp = s3c.list_objects_v2(Bucket=BUCKET, Prefix=prefix_tmp)\n        for o in resp.get(\"Contents\", []):\n            k = o[\"Key\"]\n            if re.search(r\"/part-.*\\.parquet$\", k):\n                part_key = k; break\n        if not part_key:\n            raise RuntimeError(f\"part-*.parquet não encontrado em s3://{BUCKET}/{prefix_tmp}\")\n\n        dest_key = f\"{RAW_PREF}/{base_no_ext}.parquet\"\n        s3c.copy_object(Bucket=BUCKET, CopySource={\"Bucket\": BUCKET, \"Key\": part_key}, Key=dest_key)\n\n        # limpar tmp\n        del_resp = s3c.list_objects_v2(Bucket=BUCKET, Prefix=prefix_tmp)\n        for o in del_resp.get(\"Contents\", []):\n            s3c.delete_object(Bucket=BUCKET, Key=o[\"Key\"])\n\n        print(f\"[FLATTEN] OK → s3://{BUCKET}/{dest_key}\")\n        return f\"s3://{BUCKET}/{dest_key}\"\n    else:\n        out_dir = f\"{raw_uri}/{base_no_ext}/\"\n        df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(out_dir)\n        print(f\"[WRITE] parquet (pasta): {out_dir}\")\n        return out_dir\n\n# ---------- Processamento ----------\nsrc_prefix = RAW_PREF + \"/\"\nkeys = list(list_raw_files(src_prefix))\nprint(f\"[INFO] Arquivos (.csv/.txt/.tsv) encontrados em RAW: {len(keys)}\")\nif not keys:\n    raise RuntimeError(f\"Não há arquivos em s3://{BUCKET}/{RAW_PREF}/\")\n\nok = 0; fail = 0\nfor key in keys:\n    try:\n        filename = key.rsplit(\"/\", 1)[-1]\n        base_no_ext = re.sub(r\"\\.[^.]+$\", \"\", filename)\n\n        # Se não for overwrite, pula quando já existir o parquet final\n        dest_probe = f\"{RAW_PREF}/{base_no_ext}.parquet\"\n        if not OVERWRITE:\n            try:\n                s3c.head_object(Bucket=BUCKET, Key=dest_probe)\n                print(f\"[SKIP] já existe (use OVERWRITE_EXISTING=true p/ recriar): s3://{BUCKET}/{dest_probe}\")\n                continue\n            except Exception:\n                pass\n\n        # Detecta encoding (com confiança) e depois o separador usando o mesmo sample\n        enc_detected, conf = detect_encoding_s3(key)\n        sep = infer_delimiter_s3(key, encoding_hint=enc_detected)\n        src_uri = f\"{bucket_uri}/{key}\"\n        print(f\"[FILE] {src_uri} | enc={enc_detected} (conf={conf:.2f}) | sep='{sep}' → RAW/{base_no_ext}.parquet\")\n\n        # Leitura com cascata de fallbacks\n        enc_candidates = [enc_detected, \"UTF-8\", \"ISO-8859-1\", \"windows-1252\"]\n        read_ok = False\n        last_err = None\n        for enc in enc_candidates:\n            try:\n                df = (spark.read\n                        .option(\"header\", True)\n                        .option(\"sep\", sep)\n                        .option(\"encoding\", enc)\n                        .option(\"mode\", \"PERMISSIVE\")\n                        .csv(src_uri))\n                # Se vier sem colunas (às vezes por BOM/encoding), força erro p/ tentar próximo encoding\n                if not df.columns:\n                    raise Exception(f\"Sem colunas com encoding={enc}.\")\n                read_ok = True\n                break\n            except Exception as e:\n                last_err = e\n                continue\n        if not read_ok:\n            raise last_err or Exception(\"Falha de leitura em todos os encodings testados.\")\n\n        df = normalize_columns_df(df)\n        write_parquet_in_raw(df, base_no_ext)\n\n        ok += 1\n    except Exception as e:\n        fail += 1\n        print(f\"[ERROR] Falha no arquivo {key}: {e}\")\n\nprint(f\"[SUMMARY] Sucesso: {ok} | Falhas: {fail}\")\n\n# ---------- Commit Glue (se aplicável) ----------\ntry:\n    job.commit()\nexcept Exception:\n    pass\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: dfb36435-2779-455a-b289-e487e1c1d6fe\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\nWaiting for session dfb36435-2779-455a-b289-e487e1c1d6fe to get into ready status...\nSession dfb36435-2779-455a-b289-e487e1c1d6fe has been created.\n[INFO] BUCKET=bucket-etl-edb011 | RAW=s3://bucket-etl-edb011/raw/ | NORMALIZE_COLUMNS=True | FLATTEN=True | OVERWRITE_EXISTING=True\n[INFO] Arquivos (.csv/.txt/.tsv) encontrados em RAW: 10\n[FILE] s3://bucket-etl-edb011/raw/2021_tri_01.csv | enc=windows-1252 (conf=0.73) | sep=';' → RAW/2021_tri_01.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/2021_tri_01/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/2021_tri_01.parquet\n[FILE] s3://bucket-etl-edb011/raw/2021_tri_02.csv | enc=windows-1252 (conf=0.73) | sep=';' → RAW/2021_tri_02.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/2021_tri_02/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/2021_tri_02.parquet\n[FILE] s3://bucket-etl-edb011/raw/2021_tri_03.csv | enc=windows-1252 (conf=0.73) | sep=';' → RAW/2021_tri_03.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/2021_tri_03/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/2021_tri_03.parquet\n[FILE] s3://bucket-etl-edb011/raw/2021_tri_04.csv | enc=windows-1252 (conf=0.73) | sep=';' → RAW/2021_tri_04.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/2021_tri_04/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/2021_tri_04.parquet\n[FILE] s3://bucket-etl-edb011/raw/2022_tri_01.csv | enc=windows-1252 (conf=0.73) | sep=';' → RAW/2022_tri_01.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/2022_tri_01/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/2022_tri_01.parquet\n[FILE] s3://bucket-etl-edb011/raw/2022_tri_03.csv | enc=windows-1252 (conf=0.73) | sep=';' → RAW/2022_tri_03.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/2022_tri_03/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/2022_tri_03.parquet\n[FILE] s3://bucket-etl-edb011/raw/2022_tri_04.csv | enc=windows-1252 (conf=0.73) | sep=';' → RAW/2022_tri_04.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/2022_tri_04/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/2022_tri_04.parquet\n[FILE] s3://bucket-etl-edb011/raw/EnquadramentoInicia_v2.tsv | enc=UTF-8 (conf=0.99) | sep='\t' → RAW/EnquadramentoInicia_v2.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/EnquadramentoInicia_v2/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/EnquadramentoInicia_v2.parquet\n[FILE] s3://bucket-etl-edb011/raw/glassdoor_consolidado_join_match_less_v2.csv | enc=UTF-8 (conf=0.99) | sep='|' → RAW/glassdoor_consolidado_join_match_less_v2.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/glassdoor_consolidado_join_match_less_v2/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/glassdoor_consolidado_join_match_less_v2.parquet\n[FILE] s3://bucket-etl-edb011/raw/glassdoor_consolidado_join_match_v2.csv | enc=UTF-8 (conf=0.99) | sep='|' → RAW/glassdoor_consolidado_join_match_v2.parquet\n[WRITE] tmp: s3://bucket-etl-edb011/raw/__tmp/glassdoor_consolidado_join_match_v2/\n[FLATTEN] OK → s3://bucket-etl-edb011/raw/glassdoor_consolidado_join_match_v2.parquet\n[SUMMARY] Sucesso: 10 | Falhas: 0\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Base Trusted Bancos",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# =========================\n# Enquadramento -> TRUSTED/base_bancos_TRUSTED.parquet\n# === Versão à prova de BOM em nomes de colunas + renomeação robusta ===\n# =========================\nimport re as _re\nimport unicodedata\nfrom pyspark.sql import functions as F, types as T\n\n# ---- fallbacks se variáveis não existirem (rode apenas se necessário) ----\ntry:\n    BUCKET, TRUSTED_PREFIX, RAW_PREF, spark, s3c\nexcept NameError:\n    from pyspark.sql import SparkSession\n    import boto3\n    spark = SparkSession.builder.appName(\"bancos_trusted\").getOrCreate()\n    BUCKET = \"bucket-etl-edb011\"      # ajuste se preciso\n    RAW_PREF = \"raw\"\n    TRUSTED_PREFIX = \"trusted\"\n    s3c = boto3.client(\"s3\")\n\nbucket_uri    = f\"s3://{BUCKET}\"\nraw_uri       = f\"{bucket_uri}/{RAW_PREF}\"\ntrusted_uri   = f\"{bucket_uri}/{TRUSTED_PREFIX}\"\n\n# ---------- helpers de nomes (remove BOM, acentos, espaços estranhos) ----------\ndef _clean_name(name: str) -> str:\n    if name is None:\n        return \"\"\n    # remove BOM, trims e normaliza espaços\n    name = str(name).lstrip(\"\\ufeff\").strip()\n    # normaliza acentos -> base ASCII (evita colunas com \"ç, á, ã\" etc.)\n    name = unicodedata.normalize(\"NFKD\", name)\n    name = \"\".join(ch for ch in name if not unicodedata.combining(ch))\n    # colapsa espaços internos\n    name = _re.sub(r\"\\s+\", \" \", name)\n    return name\n\ndef _lower_key(name: str) -> str:\n    # chave para comparações: lower + sem BOM + trims\n    return _clean_name(name).lower()\n\n# 1) Lê o parquet do RAW (nome-base do .tsv convertido anteriormente)\nsrc_key  = f\"{RAW_PREF}/EnquadramentoInicia_v2.parquet\"   # se o nome do arquivo diferir, ajuste aqui\nsrc_path = f\"{bucket_uri}/{src_key}\"\n\ndf = spark.read.parquet(src_path)\n\n# 1.1) normaliza *internamente* os nomes de colunas (remove BOM e espaços/acentos), mas mantendo forma original para matching\norig_cols = df.columns\nclean_map = {}\nfor c in orig_cols:\n    cc = _clean_name(c)\n    if cc != c:\n        clean_map[c] = cc\n\n# aplica renomeações \"cosméticas\" só para remover BOM/espaços/acentos do nome físico\nfor old, new in clean_map.items():\n    if new and new != old:\n        # cuidado com colisões; se já existir, pula\n        if new not in df.columns:\n            df = df.withColumnRenamed(old, new)\n\n# 2) Renomeia colunas como no seu pandas:\n#    segmento -> Segmento_Bancos\n#    cnpj     -> CNPJ_Bancos\n#    nome     -> Nome_Bancos\n# Faremos um matching case-insensitive e tolerante a espaços/BOM/acentos\nrename_targets = {\n    \"segmento\": \"Segmento_Bancos\",\n    \"cnpj\": \"CNPJ_Bancos\",\n    \"nome\": \"Nome_Bancos\",\n}\n\n# constrói índice (chave baixa e limpa -> nome atual)\nidx = {_lower_key(c): c for c in df.columns}\n\nfor k_low, final_name in rename_targets.items():\n    if k_low in idx:\n        current = idx[k_low]\n        if current != final_name:\n            # evita sobrescrever coluna existente\n            if final_name not in df.columns:\n                df = df.withColumnRenamed(current, final_name)\n\n# 3) Trata documento (equivalente ao seu trata_documento):\n#    - remove não-dígitos\n#    - se tiver <8 dígitos, faz zero-fill até 8; senão mantém como está (sem truncar)\nif \"CNPJ_Bancos\" not in df.columns:\n    raise RuntimeError(\"Coluna 'CNPJ_Bancos' não encontrada após o rename. Verifique o schema do RAW.\")\n\ndigits = F.regexp_replace(F.col(\"CNPJ_Bancos\").cast(T.StringType()), r\"[^0-9]\", \"\")\ndf = df.withColumn(\n    \"CNPJ_Bancos\",\n    F.when(F.length(digits) < 8, F.lpad(digits, 8, \"0\")).otherwise(digits)\n)\n\n# 4) (opcional) garante tipos string nas demais colunas\n#    aqui convertemos todas as colunas para StringType para evitar surpresas de encoding ao exportar/ler depois\nfor c in df.columns:\n    df = df.withColumn(c, F.col(c).cast(T.StringType()))\n\n# 5) Escreve no TRUSTED com flatten: trusted/base_bancos_TRUSTED.parquet\nfinal_name = \"base_bancos_TRUSTED\"\ntmp_dir = f\"{trusted_uri}/{final_name}/\"\ndf.coalesce(1).write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(tmp_dir)\nprint(f\"[TRUSTED] tmp gravado: {tmp_dir}\")\n\n# Copia part-*.parquet para arquivo solto e apaga a pasta temporária\nprefix_tmp = f\"{TRUSTED_PREFIX}/{final_name}/\"\npart_key = None\nresp = s3c.list_objects_v2(Bucket=BUCKET, Prefix=prefix_tmp)\nfor o in resp.get(\"Contents\", []):\n    k = o[\"Key\"]\n    if _re.search(r\"/part-.*\\.parquet$\", k):\n        part_key = k; break\nif not part_key:\n    raise RuntimeError(f\"part-*.parquet não encontrado em s3://{BUCKET}/{prefix_tmp}\")\n\ndest_key = f\"{TRUSTED_PREFIX}/{final_name}.parquet\"\ns3c.copy_object(Bucket=BUCKET, CopySource={\"Bucket\": BUCKET, \"Key\": part_key}, Key=dest_key)\n\n# limpa tmp\nfor o in s3c.list_objects_v2(Bucket=BUCKET, Prefix=prefix_tmp).get(\"Contents\", []):\n    s3c.delete_object(Bucket=BUCKET, Key=o[\"Key\"])\n\nprint(f\"[OK] TRUSTED → s3://{BUCKET}/{dest_key}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "[TRUSTED] tmp gravado: s3://bucket-etl-edb011/trusted/base_bancos_TRUSTED/\n[OK] TRUSTED → s3://bucket-etl-edb011/trusted/base_bancos_TRUSTED.parquet\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Base Trusted Empregados",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import re, unicodedata, pandas as pd\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import pandas_udf, broadcast\nimport boto3\n\n# -------- Config --------\nBUCKET = \"bucket-etl-edb011\"\nRAW_PREF = \"raw\"\nTRUSTED_PREFIX = \"trusted\"\n\nbucket_uri  = f\"s3://{BUCKET}\"\nraw_uri     = f\"{bucket_uri}/{RAW_PREF}\"\ntrusted_uri = f\"{bucket_uri}/{TRUSTED_PREFIX}\"\n\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\ns3c = boto3.client(\"s3\")\n\n# -------- Helpers de texto/encoding para usar dentro das UDFs --------\n_MOJIBAKE_BAD = (\"Ã\", \"Â\", \"ï¿½\")  # padrões comuns quando utf-8/latin1 cruza errado\n\ndef _fix_mojibake_series(s: pd.Series) -> pd.Series:\n    \"\"\"Tenta corrigir casos típicos de mojibake (latin1->utf8) e remove BOM/ZWSP.\"\"\"\n    s = s.astype(\"string\")\n    # remove BOM e zero-width\n    s = s.str.replace(\"\\ufeff\", \"\", regex=False).str.replace(\"\\u200b\", \"\", regex=False).str.strip()\n\n    # tentativa de recodificação latin1->utf8\n    def _try_fix(x: pd._libs.missing.NAType | str) -> pd._libs.missing.NAType | str:\n        if x is pd.NA or x is None:\n            return pd.NA\n        t = str(x)\n        try:\n            t1 = t.encode(\"latin1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            t1 = t\n        # heurística: se a versão “t1” tem menos artefatos, fique com ela\n        score_t  = sum(ch in _MOJIBAKE_BAD for ch in t)\n        score_t1 = sum(ch in _MOJIBAKE_BAD for ch in t1)\n        out = t1 if score_t1 < score_t else t\n        # normaliza composição unicode (NFC)\n        out = unicodedata.normalize(\"NFC\", out)\n        out = out.strip()\n        return out if out != \"\" else pd.NA\n\n    return s.apply(_try_fix)\n\n# -------- Pandas UDFs --------\n@pandas_udf(\"string\")\ndef limpar_nome_pudf(s: pd.Series) -> pd.Series:\n    \"\"\"Normaliza nomes: corrige mojibake, remove acento, caixa alta e remove termos ruidosos.\"\"\"\n    s = _fix_mojibake_series(s)\n\n    def limp(x):\n        if x is pd.NA:\n            return None\n        t = unicodedata.normalize(\"NFKD\", str(x))\n        t = \"\".join(ch for ch in t if not unicodedata.combining(ch))\n        t = t.upper()\n\n        # remoção de termos ruidosos comuns em razão social de bancos\n        remove_lista = [\n            'S/A','S.A.','S.A','BANCO','FINANCEIRA','(BRASIL)','BRASIL','MULTIPLO','CREDITO',\n            'CREDITO, FINANCIAMENTO E INVESTIMENTOS','INSTITUICAO DE PAGAMENTO','(CONGLOMERADO)',\n            'MEDIUM','FINANCEIRO','CRED','SCFI','FINANCIAMENTO','INDUSTRIAL','GRUPO','SEGURO',\n            'BANK','INVESTIMENTOS','CFI','BS2',' - PRUDENCIAL','PRUDENCIAL','INVESTIMENTO',\n            'CAPITAL','SOCIEDADE','DE'\n        ]\n        for palavra in remove_lista:\n            t = t.replace(palavra, '')\n\n        # limpa pontuação/espacos múltiplos\n        t = re.sub(r\"[.,\\-]\", \" \", t)\n        t = re.sub(r\"\\s+\", \" \", t).strip()\n        return t if t else None\n\n    return s.apply(limp)\n\n@pandas_udf(\"string\")\ndef clean_doc_pudf(s: pd.Series) -> pd.Series:\n    \"\"\"Mantém apenas dígitos e faz zero-fill até 8; vazio vira None.\"\"\"\n    s = _fix_mojibake_series(s)\n    s = s.astype(\"string\").fillna(\"\")\n    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n    return s.apply(lambda x: x.zfill(8) if x else None)\n\n@pandas_udf(\"string\")\ndef fix_text_pudf(s: pd.Series) -> pd.Series:\n    \"\"\"Correção geral de texto (BOM/ZWSP/mojibake + NFC).\"\"\"\n    out = _fix_mojibake_series(s)\n    # vazio -> None\n    return out.apply(lambda x: None if (x is pd.NA or x is None or str(x).strip() == \"\") else x)\n\ndef fix_all_string_columns(df):\n    \"\"\"Aplica fix_text_pudf em todas as colunas string (útil após joins).\"\"\"\n    for c, dtype in df.dtypes:\n        if dtype == \"string\":\n            df = df.withColumn(c, fix_text_pudf(F.col(c)))\n    return df\n\n# -------- Leitura (Parquet não tem encoding de arquivo) --------\ng1_path = f\"{raw_uri}/glassdoor_consolidado_join_match_v2.parquet\"\ng2_path = f\"{raw_uri}/glassdoor_consolidado_join_match_less_v2.parquet\"\nbancos_path = f\"{trusted_uri}/base_bancos_TRUSTED.parquet\"\n\ndf1 = spark.read.parquet(g1_path)\ndf2 = spark.read.parquet(g2_path)\ndf_bancos = spark.read.parquet(bancos_path)\n\n# -------- Normalização de texto antes do match --------\n# Corrige mojibake/BOM nas colunas relevantes\nif \"nome\" in df1.columns:\n    df1 = df1.withColumn(\"nome\", fix_text_pudf(F.col(\"nome\")))\nif \"nome\" in df2.columns:\n    df2 = df2.withColumn(\"nome\", fix_text_pudf(F.col(\"nome\")))\nif \"Nome_Bancos\" in df_bancos.columns:\n    df_bancos = df_bancos.withColumn(\"Nome_Bancos\", fix_text_pudf(F.col(\"Nome_Bancos\")))\n\n# Nome normalizado para o join\ndf1 = df1.withColumn(\"nome_normalizado\", limpar_nome_pudf(F.col(\"nome\")))\ndf2 = df2.withColumn(\"nome_normalizado\", limpar_nome_pudf(F.col(\"nome\")))\ndf_bancos = df_bancos.withColumn(\"nome_normalizado\", limpar_nome_pudf(F.col(\"Nome_Bancos\")))\n\nsel_bancos = (\n    df_bancos\n    .select(\"Nome_Bancos\", \"Segmento_Bancos\", \"CNPJ_Bancos\", \"nome_normalizado\")\n    .dropDuplicates([\"nome_normalizado\"])\n)\n\ndf_main  = df1.join(broadcast(sel_bancos), on=\"nome_normalizado\", how=\"left\").drop(\"nome_normalizado\")\ndf_main2 = df2.join(broadcast(sel_bancos), on=\"nome_normalizado\", how=\"left\").drop(\"nome_normalizado\")\n\n# -------- Ajustes pós-join --------\nif \"CNPJ_Bancos\" in df_main.columns:\n    df_main  = df_main.withColumnRenamed(\"CNPJ_Bancos\", \"cnpj\").drop(\"Segmento_Bancos\")\nif \"Segmento_Bancos\" in df_main2.columns:\n    df_main2 = df_main2.withColumnRenamed(\"Segmento_Bancos\", \"segmento\").drop(\"CNPJ_Bancos\")\n\ndf_emp = df_main.unionByName(df_main2, allowMissingColumns=True)\n\n# Limpeza geral de strings (corrige eventuais sobras de mojibake/BOM)\ndf_emp = fix_all_string_columns(df_emp)\n\n# -------- Trata CNPJ --------\nif \"cnpj\" in df_emp.columns:\n    df_emp = df_emp.withColumn(\"cnpj\", clean_doc_pudf(F.col(\"cnpj\")))\n\n# -------- Grava temporário com 1 partição --------\ntmp_prefix = f\"{TRUSTED_PREFIX}/tmp_base_empregados_TRUSTED\"\ntmp_path = f\"{bucket_uri}/{tmp_prefix}\"\n\ndf_emp.coalesce(1).write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(tmp_path)\n\n# -------- Copia part-*.parquet para nome final --------\nresp = s3c.list_objects_v2(Bucket=BUCKET, Prefix=tmp_prefix)\npart_key = None\nfor o in resp.get(\"Contents\", []):\n    if re.search(r\"part-.*\\.parquet$\", o[\"Key\"]):\n        part_key = o[\"Key\"]\n        break\n\nif part_key:\n    final_key = f\"{TRUSTED_PREFIX}/base_empregados_TRUSTED.parquet\"\n    s3c.copy_object(Bucket=BUCKET, CopySource={\"Bucket\": BUCKET, \"Key\": part_key}, Key=final_key)\n    # Apaga temporário\n    for o in resp.get(\"Contents\", []):\n        s3c.delete_object(Bucket=BUCKET, Key=o[\"Key\"])\n    print(f\"[OK] Arquivo final: s3://{BUCKET}/{final_key}\")\nelse:\n    print(\"[ERRO] Arquivo part-*.parquet não encontrado no tmp.\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'ResponseMetadata': {'RequestId': 'YZCS5Y3TSGPMRA90', 'HostId': 'O7M3riE+xGIdxILIHclflEvRDYky5ZHOB+n9rbYKysEQ9IC4PrMWxIFdD9h5Ttd6u5+opK7MKDI=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'O7M3riE+xGIdxILIHclflEvRDYky5ZHOB+n9rbYKysEQ9IC4PrMWxIFdD9h5Ttd6u5+opK7MKDI=', 'x-amz-request-id': 'YZCS5Y3TSGPMRA90', 'date': 'Mon, 11 Aug 2025 01:33:38 GMT', 'x-amz-server-side-encryption': 'AES256', 'content-type': 'application/xml', 'content-length': '275', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'ServerSideEncryption': 'AES256', 'CopyObjectResult': {'ETag': '\"64b87160bbdc1a501ce3f1056bd5314b\"', 'LastModified': datetime.datetime(2025, 8, 11, 1, 33, 38, tzinfo=tzlocal())}}\n{'ResponseMetadata': {'RequestId': 'YZCYP7QKAVVFJJA4', 'HostId': '2EAh0Iy1ZjbXS9w5yqKq0PRdoUYJfLgvUqECEyvI/HY7tXe3PEjPWKZpGOwx4n/EIExdAWwWYaw=', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-id-2': '2EAh0Iy1ZjbXS9w5yqKq0PRdoUYJfLgvUqECEyvI/HY7tXe3PEjPWKZpGOwx4n/EIExdAWwWYaw=', 'x-amz-request-id': 'YZCYP7QKAVVFJJA4', 'date': 'Mon, 11 Aug 2025 01:33:38 GMT', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n[OK] Arquivo final: s3://bucket-etl-edb011/trusted/base_empregados_TRUSTED.parquet\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Base Trusted Reclamações",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# =========================\n# Reclamações -> TRUSTED/base_reclamacoes_TRUSTED.parquet (arquivo único)\n# === Versão à prova de BOM/mojibake e com detecção robusta de colunas ===\n# =========================\nimport re, unicodedata, pandas as pd, boto3\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import pandas_udf, broadcast\n\n# ======= CONFIGS =======\nBUCKET          = \"bucket-etl-edb011\"   # ajuste se necessário\nRAW_PREFIX      = \"raw\"\nTRUSTED_PREFIX  = \"trusted\"\nTARGET_LEN_DOC  = 8          # 8 = ISPB; use 14 p/ CNPJ completo\n\n# ======= Bootstrap Spark / Arrow =======\ntry:\n    spark\nexcept NameError:\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder.appName(\"base_reclamacoes_trusted_flat\").getOrCreate()\n\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n\nbucket_uri   = f\"s3://{BUCKET}\"\nraw_uri      = f\"{bucket_uri}/{RAW_PREFIX}\"\ntrusted_uri  = f\"{bucket_uri}/{TRUSTED_PREFIX}\"\ns3c = boto3.client(\"s3\")\n\n# ======= Helpers de encoding/texto =======\n_MOJIBAKE_BAD = (\"Ã\", \"Â\", \"ï¿½\")   # artefatos comuns de latin1/utf8\n\ndef remove_acentos(s: str) -> str:\n    s = unicodedata.normalize(\"NFKD\", s)\n    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n\ndef strip_bom_zwsp(s: str) -> str:\n    return s.replace(\"\\ufeff\", \"\").replace(\"\\u200b\", \"\").strip()\n\ndef fix_mojibake_series(s: pd.Series) -> pd.Series:\n    \"\"\"Remove BOM/ZWSP e tenta corrigir mojibake latin1->utf8; normaliza para NFC.\"\"\"\n    s = s.astype(\"string\")\n    s = s.str.replace(\"\\ufeff\", \"\", regex=False).str.replace(\"\\u200b\", \"\", regex=False).str.strip()\n    def _fix(x):\n        if x is pd.NA or x is None:\n            return pd.NA\n        t = str(x)\n        try:\n            t1 = t.encode(\"latin1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            t1 = t\n        score_t  = sum(ch in _MOJIBAKE_BAD for ch in t)\n        score_t1 = sum(ch in _MOJIBAKE_BAD for ch in t1)\n        out = t1 if score_t1 < score_t else t\n        out = unicodedata.normalize(\"NFC\", out).strip()\n        return out if out != \"\" else pd.NA\n    return s.apply(_fix)\n\ndef pick_smart(df, exact_candidates, keywords_all=None):\n    \"\"\"\n    Escolhe coluna por candidatos exatos (case-insensitive; tolera acento/BOM) ou por keywords.\n    \"\"\"\n    def _clean_key(x: str) -> str:\n        return remove_acentos(strip_bom_zwsp(x)).lower()\n    idx = {_clean_key(c): c for c in df.columns}\n\n    for cand in exact_candidates:\n        k = remove_acentos(strip_bom_zwsp(cand)).lower()\n        if k in idx:\n            return idx[k]\n\n    if keywords_all:\n        keys = []\n        for c in df.columns:\n            cc = remove_acentos(strip_bom_zwsp(c)).lower()\n            if all(k in cc for k in keywords_all):\n                keys.append(c)\n        if keys:\n            return sorted(keys, key=len)[0]\n    return None\n\n@pandas_udf(\"string\")\ndef limpar_nome_pudf(s: pd.Series) -> pd.Series:\n    \"\"\"Normaliza nomes: corrige mojibake, remove acento, upper, tira termos ruidosos e pontuação.\"\"\"\n    s = fix_mojibake_series(s)\n    def limp(x):\n        if x is pd.NA:\n            return None\n        t = remove_acentos(str(x)).upper()\n        remove_lista = [\n            'S/A','S.A.','S.A','BANCO','FINANCEIRA','(BRASIL)','BRASIL','MULTIPLO','CREDITO',\n            'CREDITO, FINANCIAMENTO E INVESTIMENTOS','INSTITUICAO DE PAGAMENTO','(CONGLOMERADO)',\n            'MEDIUM','FINANCEIRO','CRED','SCFI','FINANCIAMENTO','INDUSTRIAL','GRUPO','SEGURO',\n            'BANK','INVESTIMENTOS','CFI','BS2',' - PRUDENCIAL','PRUDENCIAL','INVESTIMENTO',\n            'CAPITAL','SOCIEDADE','DE'\n        ]\n        for palavra in remove_lista:\n            t = t.replace(palavra, '')\n        t = re.sub(r\"[.,\\-]\", \" \", t)\n        t = re.sub(r\"\\s+\", \" \", t).strip()\n        return t if t else None\n    return s.apply(limp)\n\n@pandas_udf(\"string\")\ndef clean_doc_pudf(s: pd.Series) -> pd.Series:\n    \"\"\"Mantém dígitos e aplica zero-fill; vazio -> None.\"\"\"\n    s = fix_mojibake_series(s).astype(\"string\").fillna(\"\")\n    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n    return s.apply(lambda x: x.zfill(TARGET_LEN_DOC) if x else None)\n\n# ======= util S3 =======\ndef s3_key_exists(bucket: str, key: str) -> bool:\n    try:\n        s3c.head_object(Bucket=bucket, Key=key)\n        return True\n    except Exception:\n        return False\n\n# ======= 1) Leitura dos trimestres (RAW) =======\ntri_basenames = [\n    \"2021_tri_01\",\"2021_tri_02\",\"2021_tri_03\",\"2021_tri_04\",\n    \"2022_tri_01\",\"2022_tri_03\",\"2022_tri_04\",\n]\n\ntri_paths = []\nfor base in tri_basenames:\n    for cand in (f\"{RAW_PREFIX}/{base}.parquet\", f\"{RAW_PREFIX}/{base}_raw.parquet\"):\n        if s3_key_exists(BUCKET, cand):\n            tri_paths.append(f\"{bucket_uri}/{cand}\")\n            break\n    else:\n        print(f\"[WARN] não encontrei RAW/{base}.parquet nem RAW/{base}_raw.parquet\")\n\nif not tri_paths:\n    raise RuntimeError(\"Nenhum trimestre encontrado no RAW.\")\n\ndfs = [spark.read.parquet(p) for p in tri_paths]\ndf2 = dfs[0]\nfor d in dfs[1:]:\n    df2 = df2.unionByName(d, allowMissingColumns=True)\n\n# remove colunas 'unnamed' (inclui casos com BOM/acentos)\nfor c in list(df2.columns):\n    cname = remove_acentos(strip_bom_zwsp(c)).lower()\n    if cname.startswith(\"unnamed\"):\n        df2 = df2.drop(c)\n\n# ======= 2) Base de bancos (TRUSTED) =======\nbancos_path = f\"{trusted_uri}/base_bancos_TRUSTED.parquet\"\ndf_bancos   = spark.read.parquet(bancos_path)\nif \"Nome_Bancos\" not in df_bancos.columns:\n    raise RuntimeError(\"Esperava 'Nome_Bancos' em base_bancos_TRUSTED.parquet.\")\n\n# sanitiza texto de match antes de normalizar\ndf_bancos = df_bancos.withColumn(\n    \"Nome_Bancos\",\n    F.udf(lambda x: strip_bom_zwsp(x) if x is not None else None, \"string\")(F.col(\"Nome_Bancos\"))\n)\n\n# ======= 3) Detecta colunas-alvo =======\ncol_inst = pick_smart(\n    df2,\n    exact_candidates=[\n        \"instituição_financeira\",\"instituicao_financeira\",\"nome_instituicao\",\n        \"instituição financeira\",\"instituicao financeira\",\"institui_o_financeira\"\n    ],\n    keywords_all=[\"institu\",\"finan\"]\n)\ncol_cnpjif = pick_smart(\n    df2,\n    exact_candidates=[\"cnpj_if\",\"cnpjif\",\"cnpj_if_\",\"cnpj if\"],\n    keywords_all=[\"cnpj\",\"if\"]\n)\n\nif not col_inst:\n    raise RuntimeError(f\"Coluna de instituição financeira não encontrada. Colunas: {df2.columns}\")\nif not col_cnpjif:\n    raise RuntimeError(f\"Coluna 'cnpj_if' não encontrada. Colunas: {df2.columns}\")\n\nprint(f\"[PICK] instituicao_financeira = '{col_inst}'\")\nprint(f\"[PICK] cnpj_if               = '{col_cnpjif}'\")\n\n# ======= 4) Split ready / nready =======\nis_blank = (F.col(col_cnpjif).isNull()) | (F.trim(F.col(col_cnpjif)) == \"\")\ndf_ready  = df2.filter(~is_blank)\ndf_nready = df2.filter(is_blank)\n\n# elimina qualquer 'cnpj_if' residual para evitar ambiguidade no join\nfor c in list(df_nready.columns):\n    if remove_acentos(strip_bom_zwsp(c)).lower() == \"cnpj_if\":\n        df_nready = df_nready.drop(c)\n\n# ======= 5) Preparação do match (=100) =======\n# Corrige mojibake/BOM nas colunas de texto ANTES do normalizador\ndf_aux = (\n    df2.select(F.col(col_inst).alias(\"instituicao_financeira\"),\n               F.col(col_cnpjif).alias(\"cnpj_if\"))\n      .filter(is_blank)\n      .dropDuplicates([\"instituicao_financeira\"])\n)\ndf_aux = df_aux.withColumn(\n    \"instituicao_financeira\",\n    F.udf(lambda x: strip_bom_zwsp(x) if x is not None else None, \"string\")(F.col(\"instituicao_financeira\"))\n)\n\naux_norm    = df_aux.select(\"*\", limpar_nome_pudf(F.col(\"instituicao_financeira\")).alias(\"nome_norm\"))\nbancos_norm = (df_bancos\n               .select(\"Nome_Bancos\",\"CNPJ_Bancos\")\n               .select(\"*\", limpar_nome_pudf(F.col(\"Nome_Bancos\")).alias(\"nome_norm\"))\n               .dropDuplicates([\"nome_norm\"]))\n\ndf_merge = (aux_norm\n            .join(broadcast(bancos_norm.select(\"nome_norm\",\"CNPJ_Bancos\")), on=\"nome_norm\", how=\"left\")\n            .drop(\"nome_norm\")\n            .withColumnRenamed(\"CNPJ_Bancos\",\"cnpj_if_sug\"))\n\n# ======= 6) Une sugestão no nready (sem ambiguidade) =======\nleft_col = col_inst  # ex.: \"instituicao_financeira\"\ndf_nready_left = df_nready.withColumnRenamed(left_col, \"inst_left\")\n\ndf_merge_right = (\n    df_merge\n    .select(F.col(\"instituicao_financeira\").alias(\"inst_right\"), \"cnpj_if_sug\")\n)\n\ndf_nready = (\n    df_nready_left\n    .join(df_merge_right, df_nready_left[\"inst_left\"] == df_merge_right[\"inst_right\"], \"left\")\n    .drop(\"inst_right\")\n)\n\n# aplica sugestão e limpa temporários\nif \"cnpj_if\" in df_nready.columns:\n    df_nready = df_nready.withColumn(\"cnpj_if\", F.coalesce(F.col(\"cnpj_if\"), F.col(\"cnpj_if_sug\")))\nelse:\n    df_nready = df_nready.withColumnRenamed(\"cnpj_if_sug\", \"cnpj_if\")\ndf_nready = df_nready.drop(\"cnpj_if_sug\").withColumnRenamed(\"inst_left\", left_col)\n\n# ======= 7) Concat final + tratamento do documento =======\ndf_main_2 = df_ready.unionByName(df_nready, allowMissingColumns=True)\ndf_main_2 = df_main_2.withColumn(\"cnpj_if\", clean_doc_pudf(F.col(\"cnpj_if\").cast(\"string\")))\n\n# ======= 8) Escreve como ARQUIVO ÚNICO (flatten) =======\ntmp_prefix = f\"{TRUSTED_PREFIX}/__tmp_base_reclamacoes_TRUSTED\"\ntmp_path   = f\"{bucket_uri}/{tmp_prefix}\"\n\n(df_main_2\n    .coalesce(1)\n    .write\n    .mode(\"overwrite\")\n    .option(\"compression\",\"snappy\")\n    .parquet(tmp_path))\n\n# copia o part-*.parquet para o nome final e limpa tmp\nresp = s3c.list_objects_v2(Bucket=BUCKET, Prefix=tmp_prefix)\npart_key = None\nfor o in resp.get(\"Contents\", []):\n    if re.search(r\"part-.*\\.parquet$\", o[\"Key\"]):\n        part_key = o[\"Key\"]; break\n\nif not part_key:\n    raise RuntimeError(\"part-*.parquet não encontrado no tmp.\")\n\nfinal_key = f\"{TRUSTED_PREFIX}/base_reclamacoes_TRUSTED.parquet\"\ns3c.copy_object(Bucket=BUCKET, CopySource={\"Bucket\": BUCKET, \"Key\": part_key}, Key=final_key)\n\n# limpa temporários\nfor o in resp.get(\"Contents\", []):\n    s3c.delete_object(Bucket=BUCKET, Key=o[\"Key\"])\n\nprint(f\"[OK] TRUSTED (arquivo único): s3://{BUCKET}/{final_key}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "[PICK] instituicao_financeira = 'instituicao_financeira'\n[PICK] cnpj_if               = 'cnpj_if'\n[OK] TRUSTED (arquivo único): s3://bucket-etl-edb011/trusted/base_reclamacoes_TRUSTED.parquet\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Base Delivery final",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import re, unicodedata, pandas as pd, boto3\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import pandas_udf\n\n# ========================\n# CONFIGURAÇÕES\n# ========================\nBUCKET          = \"bucket-etl-edb011\"\nTRUSTED_PREFIX  = \"trusted\"\nDELIVERY_PREFIX = \"delivery\"\nFLATTEN         = True  # True = gera 1 arquivo único .parquet\nTARGET_LEN_DOC  = 8     # 8 = ISPB; use 14 para CNPJ completo, se preferir\n\n# Bootstrap Spark (caso esteja fora do Glue)\ntry:\n    spark\nexcept NameError:\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder.appName(\"base_final_delivery\").getOrCreate()\n\nbucket_uri   = f\"s3://{BUCKET}\"\ntrusted_uri  = f\"{bucket_uri}/{TRUSTED_PREFIX}\"\ndelivery_uri = f\"{bucket_uri}/{DELIVERY_PREFIX}\"\ns3c = boto3.client(\"s3\")\n\n# ========================\n# Funções de limpeza texto (à prova de BOM/mojibake)\n# ========================\n_MOJIBAKE_BAD = (\"Ã\", \"Â\", \"ï¿½\")  # artefatos comuns de latin1/utf8\n\n@pandas_udf(\"string\")\ndef fix_text_pudf(s: pd.Series) -> pd.Series:\n    \"\"\"Remove BOM/ZWSP, tenta corrigir mojibake latin1->utf8 e normaliza para NFC.\"\"\"\n    s = s.astype(\"string\")\n    # remove BOM e zero-width + trim\n    s = s.str.replace(\"\\ufeff\", \"\", regex=False).str.replace(\"\\u200b\", \"\", regex=False).str.strip()\n\n    def _fix(x):\n        if x is pd.NA or x is None:\n            return None\n        t = str(x)\n        try:\n            t1 = t.encode(\"latin1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n        except Exception:\n            t1 = t\n        # escolhe a versão com menos artefatos típicos\n        score_t  = sum(ch in _MOJIBAKE_BAD for ch in t)\n        score_t1 = sum(ch in _MOJIBAKE_BAD for ch in t1)\n        out = t1 if score_t1 < score_t else t\n        out = unicodedata.normalize(\"NFC\", out).strip()\n        return out if out != \"\" else None\n\n    return s.apply(_fix)\n\ndef fix_all_string_columns(df):\n    \"\"\"Aplica fix_text_pudf em todas as colunas string (bom contra BOM/ZWSP/mojibake).\"\"\"\n    for c, dtype in df.dtypes:\n        if dtype == \"string\":\n            df = df.withColumn(c, fix_text_pudf(F.col(c)))\n    return df\n\n@pandas_udf(\"string\")\ndef clean_doc_pudf(s: pd.Series) -> pd.Series:\n    \"\"\"Mantém apenas dígitos e faz zero-fill; vazio -> None.\"\"\"\n    s = s.astype(\"string\").fillna(\"\")\n    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n    return s.apply(lambda x: x.zfill(TARGET_LEN_DOC) if x else None)\n\n# ========================\n# Ler bases TRUSTED\n# ========================\ndf_main   = spark.read.parquet(f\"{trusted_uri}/base_empregados_TRUSTED.parquet\")\ndf_main_2 = spark.read.parquet(f\"{trusted_uri}/base_reclamacoes_TRUSTED.parquet\")\n\n# ========================\n# Preparar DataFrames\n# ========================\ndf_final_1 = df_main\ndf_final_2 = df_main_2.withColumnRenamed(\"cnpj_if\", \"cnpj\")\n\n# Sanear CNPJ/ISPB antes do join (evita diferenças por encoding/whitespace)\nif \"cnpj\" in df_final_1.columns:\n    df_final_1 = df_final_1.withColumn(\"cnpj\", clean_doc_pudf(F.col(\"cnpj\").cast(\"string\")))\nif \"cnpj\" in df_final_2.columns:\n    df_final_2 = df_final_2.withColumn(\"cnpj\", clean_doc_pudf(F.col(\"cnpj\").cast(\"string\")))\n\n# ========================\n# Join outer no campo CNPJ\n# ========================\ndf_final = df_final_1.join(df_final_2, on=[\"cnpj\"], how=\"outer\")\n\n# ========================\n# Corrigir encoding (todas as strings)\n# ========================\ndf_final = fix_all_string_columns(df_final)\n\n# ========================\n# Salvar na pasta DELIVERY\n# ========================\nif not FLATTEN:\n    (df_final\n        .write\n        .mode(\"overwrite\")\n        .option(\"compression\", \"snappy\")\n        .parquet(f\"{delivery_uri}/base_final_delivery\"))\n    print(f\"[OK] Pasta Parquet: {delivery_uri}/base_final_delivery\")\nelse:\n    tmp_prefix = f\"{DELIVERY_PREFIX}/__tmp_base_final\"\n    tmp_path   = f\"{bucket_uri}/{tmp_prefix}\"\n\n    (df_final\n        .coalesce(1)\n        .write\n        .mode(\"overwrite\")\n        .option(\"compression\", \"snappy\")\n        .parquet(tmp_path))\n\n    # Localiza o part-*.parquet e copia para o nome final (arquivo único)\n    resp = s3c.list_objects_v2(Bucket=BUCKET, Prefix=tmp_prefix)\n    part_key = None\n    for o in resp.get(\"Contents\", []):\n        if re.search(r\"part-.*\\.parquet$\", o[\"Key\"]):\n            part_key = o[\"Key\"]\n            break\n\n    if not part_key:\n        raise RuntimeError(\"part-*.parquet não encontrado no tmp.\")\n\n    final_key = f\"{DELIVERY_PREFIX}/base_final_delivery.parquet\"\n    s3c.copy_object(Bucket=BUCKET, CopySource={\"Bucket\": BUCKET, \"Key\": part_key}, Key=final_key)\n\n    # Limpa temporários\n    for o in resp.get(\"Contents\", []):\n        s3c.delete_object(Bucket=BUCKET, Key=o[\"Key\"])\n\n    print(f\"[OK] Arquivo único: s3://{BUCKET}/{final_key}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "{'ResponseMetadata': {'RequestId': 'M94PM6N5R0VK4364', 'HostId': '0q+WRSfV+HhDdCCMV5Odca4V7gyiyrdHSox9EQt9acS+RlYCzWVvaCO3MdZdhISpnfQSTZ7PYIQ=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': '0q+WRSfV+HhDdCCMV5Odca4V7gyiyrdHSox9EQt9acS+RlYCzWVvaCO3MdZdhISpnfQSTZ7PYIQ=', 'x-amz-request-id': 'M94PM6N5R0VK4364', 'date': 'Mon, 11 Aug 2025 01:39:00 GMT', 'x-amz-server-side-encryption': 'AES256', 'content-type': 'application/xml', 'content-length': '275', 'server': 'AmazonS3'}, 'RetryAttempts': 0}, 'ServerSideEncryption': 'AES256', 'CopyObjectResult': {'ETag': '\"2af978aa1d6a72eb0e4a8be2438ed873\"', 'LastModified': datetime.datetime(2025, 8, 11, 1, 39, tzinfo=tzlocal())}}\n{'ResponseMetadata': {'RequestId': 'AJ45HGJCDSDC4B1V', 'HostId': '1N1FxXSqnyoB8BcFtxCVPO6IFiHgErDVh8O552/UZKY4+yodrVa4HhJ5J9I6LQScKnu9wJrERvY=', 'HTTPStatusCode': 204, 'HTTPHeaders': {'x-amz-id-2': '1N1FxXSqnyoB8BcFtxCVPO6IFiHgErDVh8O552/UZKY4+yodrVa4HhJ5J9I6LQScKnu9wJrERvY=', 'x-amz-request-id': 'AJ45HGJCDSDC4B1V', 'date': 'Mon, 11 Aug 2025 01:39:01 GMT', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}\n[OK] Arquivo único: s3://bucket-etl-edb011/delivery/base_final_delivery.parquet\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# lê o parquet\ndf = spark.read.parquet(\"s3://bucket-etl-edb011/delivery/base_final_delivery.parquet\")\n\n# gera um CSV de amostra para inspeção\nMAX_ROWS = 200_000\n(df.limit(MAX_ROWS)\n   .coalesce(1)\n   .write.mode(\"overwrite\")\n   .option(\"header\", True)\n   .csv(\"s3://bucket-etl-edb011/tmp/base_final_teste_csv/\"))\n\nprint(\"Baixe em: s3://bucket-etl-edb011/tmp/base_final_teste_csv/\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Baixe em: s3://bucket-etl-edb011/tmp/base_final_teste_csv/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Salvando em arquivo MySQL",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import os\nimport sqlite3\n\n# Caminho do arquivo SQLite na pasta delivery\ndb_path = \"s3://bucket-etl-edb011/delivery/base_final_delivery.db\"\n\n# Garante que a pasta existe\nos.makedirs(os.path.dirname(db_path), exist_ok=True)\n\n# Conecta ao banco (cria se não existir)\nconn = sqlite3.connect(db_path)\n\n# Salva o DataFrame no banco\n# Aqui assumo que df_final é um DataFrame Pandas\ndf_final.toPandas().to_sql(\"base_final\", conn, if_exists=\"replace\", index=False)\n\n# Fecha conexão\nconn.close()\n\nprint(f\"[OK] Banco SQLite criado em: {db_path}\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "[OK] Banco SQLite criado em: s3://bucket-etl-edb011/delivery/base_final_delivery.db\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}